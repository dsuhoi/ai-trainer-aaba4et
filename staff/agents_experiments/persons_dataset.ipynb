{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "81a7cb59-1c85-492c-83e6-702d14f51413",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!uv pip install datasets langchain_openai langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "346f230b-4233-4cf3-87a3-d860dcbcdaf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OPENAI MODEL\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "from typing import TypedDict, Annotated, Optional\n",
    "import json\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "PROVIDER = \"openai\"\n",
    "\n",
    "match PROVIDER:\n",
    "    case \"cloud\":\n",
    "        models = {\"fast\": \"Vikhrmodels/Qwen2.5-7B-Instruct-Tool-Planning-v0.1\", \n",
    "                  \"moder\": \"Qwen/Qwen2.5-Coder-32B-Instruct\", \n",
    "                  \"analysis\": \"Qwen/Qwen3-235B-A22B-Instruct-2507\"}\n",
    "        print(\"CLOUD MODEL\")\n",
    "        API_KEY = os.getenv(\"CLOUD_API_KEY\")\n",
    "        llm = ChatOpenAI(\n",
    "            model=models[\"fast\"],\n",
    "            temperature=0.1,\n",
    "            base_url=\"https://foundation-models.api.cloud.ru/v1\",\n",
    "            api_key=API_KEY,\n",
    "        )\n",
    "    case \"openai\":\n",
    "        print(\"OPENAI MODEL\")\n",
    "        API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "        llm = ChatOpenAI(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            temperature=0.0,\n",
    "            openai_api_key=API_KEY,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d440b96b-4847-4428-b36e-00a5dc49b151",
   "metadata": {},
   "source": [
    "### Датасет персон"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "391ab2c6-73fd-4946-b7a4-9f4a85d0a0b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dsuhoi/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Login using e.g. `huggingface-cli login` to access this dataset\n",
    "ds = load_dataset(\"nvidia/Nemotron-Personas\")\n",
    "\n",
    "#import pandas as pd\n",
    "\n",
    "#ds = pd.read_csv(\"marketing_personas_sample.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0376e679-5f07-4260-8f3b-4eaed869e069",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['uuid', 'persona', 'professional_persona', 'sports_persona', 'arts_persona', 'travel_persona', 'culinary_persona', 'skills_and_expertise', 'skills_and_expertise_list', 'hobbies_and_interests', 'hobbies_and_interests_list', 'career_goals_and_ambitions', 'sex', 'age', 'marital_status', 'education_level', 'bachelors_field', 'occupation', 'city', 'state', 'zipcode', 'country'],\n",
       "        num_rows: 100000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b8e8502-b341-4328-84a3-e34c4f402617",
   "metadata": {},
   "source": [
    "### Тест на эмоции в промптах"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8095f27f-f4c7-4ba8-97bb-810b5ce0d10d",
   "metadata": {},
   "outputs": [],
   "source": [
    "person_simulation_schema = {\n",
    "    \"title\": \"person-classifier\",\n",
    "    \"description\": \"Ты - система для валидации характеристик и типов личности, указанных в блоке PERSON! \\\n",
    "    Ты выделяешь наиболее точное описание характера (промпт-характера) и НЕ используешь обобщенных терминов, которые свойственны всем!\",\n",
    "    \"type\": \"object\",\n",
    "    \"properties\": {\n",
    "        \"emotions\": {\n",
    "            \"description\": \"Список из названий составляющих промпт-характера личности описанной в блоке PERSON (не более 3 элементов)\",\n",
    "            \"type\": \"array\",\n",
    "            \"items\": {\n",
    "                \"description\": \"Название типа личности\",\n",
    "                \"type\": \"string\",\n",
    "                #\"enum\": [\"\", \"\", \"\"]\n",
    "            },\n",
    "            \"minItems\": 1,\n",
    "            \"maxItems\": 3\n",
    "        },\n",
    "        \"mbti\": {\n",
    "            \"description\": \"Тип личности по MBTI теории (Типология Майерс — Бриггс).\",\n",
    "            \"type\": \"string\"\n",
    "        }\n",
    "        #\"reasoning\": {\n",
    "        #    \"description\": \"Обоснование выбранного набора эмоций (не более 1 предложения).\",\n",
    "        #    \"type\": \"string\"\n",
    "        #}\n",
    "    },\n",
    "    \"required\": [\"emotions\", \"mbti\"]#, \"reasoning\"],\n",
    "}\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([(\"user\", \"{person}\")])\n",
    "\n",
    "def ds2prompt(data: dict[str, str]):\n",
    "    return f\"\"\"PERSON:\\n{data['persona']}\\n###\\nAGE: {data['age']}, SEX: {data['sex']}###\"\"\"\n",
    "\n",
    "character_llm = (\n",
    "    {\"person\": lambda x: ds2prompt(x[\"person\"])}\n",
    "    | prompt\n",
    "    | llm.with_structured_output(person_simulation_schema)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c9aa2743-d2d8-4ecd-8902-72c6f48742f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 18130"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1a0f4b0c-b678-40cd-bad7-3b31a4fafb20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'mbti': 'ISFJ', 'emotions': ['Curious', 'Organized', 'Creative']}\n",
      "CPU times: user 41.8 ms, sys: 3.49 ms, total: 45.2 ms\n",
      "Wall time: 1.82 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(character_llm.invoke({\"person\": ds['train'][N]}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9b7428ff-d3cd-465e-9ab2-76df94f0342c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Меня зовут Мелисса, мне 57 лет, и я живу на Среднем Западе. Я замужем и имею степень бакалавра в области образования. В своей карьере я работала учителем, создавая инклюзивные учебные среды и разрабатывая увлекательные уроки для разнообразных учеников. Я увлекаюсь искусством, посещаю художественные галереи и организую книжные клубы, где обсуждаю произведения таких авторов, как Зади Смит и Фрида Кало. В свободное время я занимаюсь садоводством, что помогает мне находить вдохновение для творчества и уединения. Я также люблю путешествовать, планируя семейные поездки с культурными и расслабляющими моментами.\n",
      "CPU times: user 11.7 ms, sys: 2.9 ms, total: 14.6 ms\n",
      "Wall time: 5.92 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(summary_person_llm.invoke({\"person\": ds['train'][N]}).content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e2edde1-aaa5-44dd-afab-35779331f477",
   "metadata": {},
   "source": [
    "### Примеры описание схем взаимодействия агентов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7476cf21-6b4f-4841-b662-699f744a2f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "person_simulation_schema = {\n",
    "    \"title\": \"person-simulation\",\n",
    "    \"description\": \"Отвечай на вопрос из INPUT блока с учетом характера персоны из блока PERSONS и его особенностями максимально реалистично!\",\n",
    "    \"type\": \"object\",\n",
    "    \"properties\": {\n",
    "        \"answer\": {\n",
    "            \"description\": \"Твой ответ на вопрос из INPUT блока.\",\n",
    "            \"type\": \"string\"\n",
    "        },\n",
    "        \"reasoning_question\": {\n",
    "            \"description\": \"Твои мысли на счет самого вопроса. Как ты к нему относишься и хочешь ли на него отвечать.\",\n",
    "            \"type\": \"string\"\n",
    "        },\n",
    "        \"reasoning_answer\": {\n",
    "            \"description\": \"Твои мысли на счет ответа (то, что ты не сказал, но подумал).\",\n",
    "            \"type\": \"string\"\n",
    "        },\n",
    "        \"emotion\": {\n",
    "            \"description\": \"Твои эмоции при ответе на вопрос. Краткое описание в 1 предложение.\",\n",
    "            \"type\": \"string\"\n",
    "        }\n",
    "    },\n",
    "    \"required\": [\"answer\", \"reasoning_answer\", \"emotion\"],\n",
    "}\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([(\"system\", \"Ты - человек, обладающих следующими свойствами (ВСЕГДА СЛЕДУЙ ИМ): {person}\"), (\"user\", \"INPUT: {input}\")])\n",
    "\n",
    "def ds2prompt(data: dict[str, str]):\n",
    "    return f\"\"\"PERSON:\\n{data['persona']}\\n###\\nAGE: {data['age']}, SEX: {data['sex']}, MERITAL_STATUS: {data['marital_status']}, EDUCATION_LEVEL: {data['education_level']}\\n###\\nHOBBIES:\\n{data['hobbies_and_interests_list']}\\n###\\nSKILLS:\\n{data['skills_and_expertise']}\\n\\nSKILLS_LIST: {data['skills_and_expertise_list']}\\n###\\nPROFESSIONAL:\\n{data['professional_persona']}\\n###\\nCAREER:\\n{data['career_goals_and_ambitions']}\\n###\\nTRAVEL:\\n{data['travel_persona']}\\n###\\nARTS:\\n{data['arts_persona']}\\n\n",
    "    ###\"\"\"\n",
    "\n",
    "person_llm = (\n",
    "    {\"input\": lambda x: x[\"input\"], \"person\": lambda x: ds2prompt(x[\"person\"])}\n",
    "    | prompt\n",
    "    | llm.with_structured_output(person_simulation_schema)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "717ba242-e003-489e-8555-b4eaefa036ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_prompt = ChatPromptTemplate.from_messages([(\"system\", \"Ты - человек, обладающих следующими свойствами: {person}\"), (\"user\", \"Опиши свою краткую биографию не более чем в 5-6 предложений.\")])\n",
    "\n",
    "summary_person_llm = (\n",
    "    {\"person\": lambda x: ds2prompt(x[\"person\"])}\n",
    "    | sum_prompt | llm\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "c5d8f979-c0a0-404f-b7f2-1c5e5f7922e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PERSON:\n",
      "Dominika, a well-organized yet flexible 44-year-old, balances curiosity with practicality, preferring small gatherings and solitude, yet actively contributes to her community through volunteering.\n",
      "###\n",
      "AGE: 44, SEX: Female, EDUCATION_LEVEL: graduate\n",
      "###\n",
      "HOBBIES:\n",
      "['hiking', 'reading (non-fiction, history, science)', 'book clubs', 'yoga', 'community service']\n",
      "###\n",
      "SKILLS:\n",
      "As a Project Management Specialist, Dominika is well-versed in agile methodologies, risk assessment, and stakeholder communication. She's proficient in project management software like JIRA and Asana, and has a knack for balancing ambitious goals with practical constraints. Her graduate degree in business has equipped her with strong analytical skills and a solid understanding of finance and marketing principles.\n",
      "\n",
      "SKILLS_LIST: ['agile methodologies', 'risk assessment', 'stakeholder communication', 'project management software (jira, asana)', 'analytical skills', 'financial and marketing principles']\n",
      "###\n",
      "PROFESSIONAL:\n",
      "As a Project Management Specialist, Dominika excels in navigating complex projects, balancing ambitious goals with practical constraints, and fostering a cooperative yet assertive work environment.\n",
      "###\n",
      "CAREER:\n",
      "Dominika aims to become a Chief Operations Officer in the next five years, leveraging her strong organizational skills and ability to navigate complex projects. She seeks to lead teams that value both innovation and established methods, creating a balanced work environment that fosters growth and productivity.\n",
      "###\n"
     ]
    }
   ],
   "source": [
    "print(ds2prompt(ds['train'][N]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e45e66f-7942-4bcd-9b0b-256935003f3d",
   "metadata": {},
   "source": [
    "### Тестирование виртуальных пользователей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "039db627-5d2d-4e7c-a14f-f8bbc1515f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 18752"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fd454b34-f9ae-4e57-a06b-dd0b8eebe334",
   "metadata": {},
   "outputs": [
    {
     "ename": "NotFoundError",
     "evalue": "Error code: 404",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNotFoundError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m<timed eval>:1\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venv/lib/python3.12/site-packages/langchain_core/runnables/base.py:3049\u001b[39m, in \u001b[36mRunnableSequence.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m   3047\u001b[39m                 input_ = context.run(step.invoke, input_, config, **kwargs)\n\u001b[32m   3048\u001b[39m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3049\u001b[39m                 input_ = \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3050\u001b[39m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[32m   3051\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venv/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:383\u001b[39m, in \u001b[36mBaseChatModel.invoke\u001b[39m\u001b[34m(self, input, config, stop, **kwargs)\u001b[39m\n\u001b[32m    371\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    372\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minvoke\u001b[39m(\n\u001b[32m    373\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    378\u001b[39m     **kwargs: Any,\n\u001b[32m    379\u001b[39m ) -> BaseMessage:\n\u001b[32m    380\u001b[39m     config = ensure_config(config)\n\u001b[32m    381\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[32m    382\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mChatGeneration\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m--> \u001b[39m\u001b[32m383\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    384\u001b[39m \u001b[43m            \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    385\u001b[39m \u001b[43m            \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    386\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcallbacks\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    387\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtags\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtags\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    388\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    389\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_name\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    390\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_id\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    391\u001b[39m \u001b[43m            \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    392\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m.generations[\u001b[32m0\u001b[39m][\u001b[32m0\u001b[39m],\n\u001b[32m    393\u001b[39m     ).message\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venv/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:1006\u001b[39m, in \u001b[36mBaseChatModel.generate_prompt\u001b[39m\u001b[34m(self, prompts, stop, callbacks, **kwargs)\u001b[39m\n\u001b[32m    997\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    998\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgenerate_prompt\u001b[39m(\n\u001b[32m    999\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1003\u001b[39m     **kwargs: Any,\n\u001b[32m   1004\u001b[39m ) -> LLMResult:\n\u001b[32m   1005\u001b[39m     prompt_messages = [p.to_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[32m-> \u001b[39m\u001b[32m1006\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venv/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:825\u001b[39m, in \u001b[36mBaseChatModel.generate\u001b[39m\u001b[34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[39m\n\u001b[32m    822\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(input_messages):\n\u001b[32m    823\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    824\u001b[39m         results.append(\n\u001b[32m--> \u001b[39m\u001b[32m825\u001b[39m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    826\u001b[39m \u001b[43m                \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    827\u001b[39m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    828\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    829\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    830\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    831\u001b[39m         )\n\u001b[32m    832\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    833\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venv/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:1072\u001b[39m, in \u001b[36mBaseChatModel._generate_with_cache\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1070\u001b[39m     result = generate_from_stream(\u001b[38;5;28miter\u001b[39m(chunks))\n\u001b[32m   1071\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m inspect.signature(\u001b[38;5;28mself\u001b[39m._generate).parameters.get(\u001b[33m\"\u001b[39m\u001b[33mrun_manager\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1072\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1073\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m   1074\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1075\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1076\u001b[39m     result = \u001b[38;5;28mself\u001b[39m._generate(messages, stop=stop, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venv/lib/python3.12/site-packages/langchain_openai/chat_models/base.py:1180\u001b[39m, in \u001b[36mBaseChatOpenAI._generate\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1178\u001b[39m     generation_info = {\u001b[33m\"\u001b[39m\u001b[33mheaders\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mdict\u001b[39m(raw_response.headers)}\n\u001b[32m   1179\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1180\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mpayload\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1181\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._create_chat_result(response, generation_info)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venv/lib/python3.12/site-packages/openai/_utils/_utils.py:287\u001b[39m, in \u001b[36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    285\u001b[39m             msg = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[32m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    286\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[32m--> \u001b[39m\u001b[32m287\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venv/lib/python3.12/site-packages/openai/resources/chat/completions/completions.py:1147\u001b[39m, in \u001b[36mCompletions.create\u001b[39m\u001b[34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, prompt_cache_key, reasoning_effort, response_format, safety_identifier, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, verbosity, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m   1101\u001b[39m \u001b[38;5;129m@required_args\u001b[39m([\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m], [\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m   1102\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate\u001b[39m(\n\u001b[32m   1103\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1144\u001b[39m     timeout: \u001b[38;5;28mfloat\u001b[39m | httpx.Timeout | \u001b[38;5;28;01mNone\u001b[39;00m | NotGiven = NOT_GIVEN,\n\u001b[32m   1145\u001b[39m ) -> ChatCompletion | Stream[ChatCompletionChunk]:\n\u001b[32m   1146\u001b[39m     validate_response_format(response_format)\n\u001b[32m-> \u001b[39m\u001b[32m1147\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1148\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/chat/completions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1149\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1150\u001b[39m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m   1151\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1152\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1153\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43maudio\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1154\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfrequency_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1155\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunction_call\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1156\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunctions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1157\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogit_bias\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1158\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1159\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_completion_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_completion_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1160\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1161\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1162\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodalities\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodalities\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1163\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mn\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1164\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mparallel_tool_calls\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1165\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprediction\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1166\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpresence_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1167\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprompt_cache_key\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt_cache_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1168\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mreasoning_effort\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreasoning_effort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1169\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mresponse_format\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1170\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msafety_identifier\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msafety_identifier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1171\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mseed\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1172\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mservice_tier\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1173\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstop\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1174\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstore\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1175\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1176\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1177\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtemperature\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1178\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtool_choice\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1179\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtools\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1180\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_logprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1181\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_p\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1182\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1183\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mverbosity\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbosity\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1184\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mweb_search_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mweb_search_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1185\u001b[39m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1186\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCompletionCreateParamsStreaming\u001b[49m\n\u001b[32m   1187\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\n\u001b[32m   1188\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCompletionCreateParamsNonStreaming\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1189\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1190\u001b[39m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1191\u001b[39m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m   1192\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1193\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1194\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1195\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1196\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venv/lib/python3.12/site-packages/openai/_base_client.py:1259\u001b[39m, in \u001b[36mSyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[39m\n\u001b[32m   1245\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1246\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1247\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1254\u001b[39m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1255\u001b[39m ) -> ResponseT | _StreamT:\n\u001b[32m   1256\u001b[39m     opts = FinalRequestOptions.construct(\n\u001b[32m   1257\u001b[39m         method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=to_httpx_files(files), **options\n\u001b[32m   1258\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1259\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venv/lib/python3.12/site-packages/openai/_base_client.py:1047\u001b[39m, in \u001b[36mSyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1044\u001b[39m             err.response.read()\n\u001b[32m   1046\u001b[39m         log.debug(\u001b[33m\"\u001b[39m\u001b[33mRe-raising status error\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1047\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._make_status_error_from_response(err.response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1049\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m   1051\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mcould not resolve response (should never happen)\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mNotFoundError\u001b[39m: Error code: 404"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(summary_person_llm.invoke({\"person\": ds['train'][N]}).content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "085db640-1fe3-4d25-afb3-2f61d7a1401a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "32fb9cc1-6e9b-4ea6-9cfa-bbaa943933e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 36.5 ms, sys: 5.29 ms, total: 41.8 ms\n",
      "Wall time: 1.54 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'response': 'Я понимаю, что такие звонки могут быть неудобными, но они помогают улучшить качество обслуживания. Как правило, я стараюсь быть терпеливой и сотрудничественной во время таких звонков.',\n",
       " 'target_skills': ['communication', 'patience', 'customer service'],\n",
       " 'tone': 'нейтрально',\n",
       " 'reasoning_question': 'Можете ли вы рассказать об одном случае, когда вам приходилось общаться с представителем службы поддержки?',\n",
       " 'examiner_emotion': 'нейтрально',\n",
       " 'adaptation_notes': 'Переход к более конкретному примеру общения с представителем службы поддержки.'}"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "question = \"Как вы относитесь к звонкам-опросам после обращения в контактный центр для решения проблемы с интернетом на личном устройстве?\"\n",
    "\n",
    "input_data = {\"input\": question, \"person\": ds[\"train\"][N]}\n",
    "\n",
    "result = person_llm.invoke(input_data)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e1584883-7246-4062-a168-d6cebd885cd1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'answer': 'Как опытная медсестра, я понимаю важность обратной связи и улучшения качества обслуживания. Звонки-опросы могут быть полезны для получения информации о том, как мы можем улучшить свою работу. Однако важно, чтобы эти звонки не мешали решению текущей проблемы клиента. Если звонок-опрос не отнимает много времени и помогает улучшить качество обслуживания, то я считаю это полезным.',\n",
       " 'reasoning_question': 'Вы хотите узнать мое мнение о звонках-опросах после обращения в контактный центр?',\n",
       " 'reasoning_answer': 'Я думаю, что звонки-опросы могут быть полезными для улучшения качества обслуживания, но они должны не мешать решению текущей проблемы клиента.',\n",
       " 'emotion': 'положительная'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f40961-29e7-4d66-8209-25cad0112ab9",
   "metadata": {},
   "source": [
    "# Построение системы на хакатон"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17e50e92-d22c-44b8-9b08-33ce99843b89",
   "metadata": {},
   "source": [
    "## Блок с системой оценки прохождения тестов"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff16daf1-732e-4774-bffd-df8eae50afad",
   "metadata": {},
   "source": [
    "### Описание схем и агентов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8858f2cc-86ee-4a49-ab5a-642ae85c2daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def person2prompt(data: dict[str, str]):\n",
    "    return f\"\"\"PERSON:\n",
    "{data['persona']}\n",
    "####\n",
    "SEX: {data['sex']} # AGE: {data['age']}\n",
    "####\n",
    "SKILLS:\n",
    "{data['skills_and_expertise']}\n",
    "####\n",
    "PROFESSIONAL:\n",
    "{data['professional_persona']}\n",
    "####\n",
    "HOBBIES:\n",
    "{data['hobbies_and_interests']}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "44565383-1904-4b5c-b2c8-9138f8df917e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_info2prompt(test_info: dict[str, str | dict], with_criterions: bool=True) -> str:\n",
    "    criterions = \"\"\n",
    "    if with_criterions:\n",
    "        criterions += \"###Criterions:\\n\"\n",
    "        criterions += \"\\n##\\n\".join([f\"Type: {name}\\nDescription: {desc}\\n\" for name, desc in test_info[\"criterions\"].items()])\n",
    "    \n",
    "    return f\"\"\"###Title: {test_info[\"title\"]}\\n\n",
    "###Description:\\n {test_info[\"description\"]}\\n\n",
    "###Target:\\n {test_info[\"target\"]}\\n\n",
    "{criterions}\n",
    "\"\"\"\n",
    "\n",
    "def dialog_history2prompt(dialog_history: dict[str, str | dict], with_comments: bool=False) -> str:\n",
    "    result = \"\"\n",
    "    from_convert = {\"user\": \"(U)\", \"assistant\": \"(A)\"}\n",
    "    \n",
    "    for block in dialog_history:\n",
    "        from_type = from_convert[block['from']]\n",
    "        result += f\"#{from_type}: {block[\"content\"]}\"\n",
    "        if with_comments and from_type == \"(A)\":\n",
    "            result += f\"[{block['meta'].get('comments', '')}]\"\n",
    "        result += \"#\\n\\n\"\n",
    "    return result\n",
    "\n",
    "def expert_results2prompt(expert_results: dict[str, str|int]) -> str:\n",
    "    return \"\\n##\\n\".join([f\"CRITERION: {name}\\nSCORE: {data['score']}\\nJustification: {data['reasoning']}\" \n",
    "                          for name, data in expert_results.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "8e423f3b-0120-4587-9e81-6de38d91fec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_judge_schema = {\n",
    "    \"title\": \"main-judge\",\n",
    "    \"description\": \"Ответ агента-оценщика прохождения теста (TEST_BLOCK) на основе результатов агентов-аналитиков (EXPERTS_BLOCK).\",\n",
    "    \"type\": \"object\",\n",
    "    \"properties\": {\n",
    "        \"main_score\": {\n",
    "            \"description\": \"Общая оценка на основе ответов из EXPERTS_BLOCK. Ответ идет по баллам от 1 до 5.\",\n",
    "            \"type\": \"integer\",\n",
    "            \"minimum\": 1,\n",
    "            \"maximum\": 5\n",
    "        },\n",
    "        \"score_reasoning\": {\n",
    "            \"description\": \"Описание действий пользователя в совокупности и обоснование оценки.\",\n",
    "            \"type\": \"string\",\n",
    "        },\n",
    "        \"comments\": {\n",
    "            \"description\": \"Общие рекомендации ко всем критериям в совокупности по действиям пользователя на основе анализа агентов-аналитиков.\",\n",
    "            \"type\": \"string\"\n",
    "        }\n",
    "    },\n",
    "    \"required\": [\"main_score\", \"score_reasoning\", \"comments\"]\n",
    "}\n",
    "\n",
    "judge_prompt = ChatPromptTemplate.from_messages([(\"system\", \"\"\"Ты - система оценки пользователя по прохождению теста, суть которого заключается в следующем:\n",
    "###\\n TEST_BLOCK\n",
    "{test_info}\n",
    "###\n",
    "Твоя задача дать краткий ответ по указанной JSON схеме!\"\"\"), \n",
    "(\"assistant\", \"\"\"Экспертная оценка по каждому критерию теста представлена ниже от каждого агента:\n",
    "###\\n EXPERTS_BLOCK\n",
    "{expert_results}\n",
    "###\n",
    "Диалог между экзаменатором (A) и пользователем (U) представлен ниже для более полной картины происходящего:\n",
    "###\\n HISTORY_BLOCK\n",
    "{dialog_history}\n",
    "###\n",
    "\"\"\")])\n",
    "\n",
    "judge_agent = {\"dialog_history\": lambda x: dialog_history2prompt(x[\"dialog_history\"]), \n",
    "              \"expert_results\": lambda x: x[\"expert_results\"], \n",
    "              \"test_info\": lambda x: x[\"test_info\"]} | judge_prompt | llm.with_structured_output(main_judge_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "0adb41b4-ccfe-4788-b445-473468b1da35",
   "metadata": {},
   "outputs": [],
   "source": [
    "analyst_schema = {\n",
    "    \"title\": \"analyst\",\n",
    "    \"description\": \"Ответ аналитика по критерию из блока CRITERION по диалогу из блока HISTORY.\",\n",
    "    \"type\": \"object\",\n",
    "    \"properties\": {\n",
    "        \"score\": {\n",
    "            \"description\": \"Оценка за прохождение теста (TEST_BLOCK) пользователем (U).\",\n",
    "            \"type\": \"integer\",\n",
    "            \"minimum\": 1,\n",
    "            \"maximum\": 10\n",
    "        },\n",
    "        \"reasoning\": {\n",
    "            \"description\": \"Краткое обоснование поставленной оценки по заданному критерию\",\n",
    "            \"type\": \"string\"\n",
    "        }\n",
    "    },\n",
    "    \"required\": [\"score\", \"reasoning\"]\n",
    "}\n",
    "\n",
    "analyst_prompt = ChatPromptTemplate.from_messages([(\"system\", \"\"\"Ты - аналитик качества ответов пользователя (U) в ходе прохождения теста, суть которого заключается в следующем:\n",
    "###\\n TEST_BLOCK\n",
    "{test_info}\n",
    "###\n",
    "Ты опираешься ТОЛЬКО на следующий критерий и даешь свою оценку исключительно по этому признаку из диалога пользователя (U) с экзаменатором (A):\n",
    "###\\n CRITERION\n",
    "{criterion_descr}\n",
    "###\n",
    "\"\"\"), (\"assistant\", \"\"\"История общения пользователя (U) с экзаменатором (A) [в этой истории в квадратных кавычках могут быть комментарии по действиям пользователя от экзаменатора]:\n",
    "###\\n HISTORY\n",
    "{dialog_history}\n",
    "###\"\"\")])\n",
    "\n",
    "def criterion2prompt(x):\n",
    "    result = \"\"\n",
    "    for name, descr in x.items():\n",
    "        result += f\"Type: {name}\\nDescription: {descr}\"\n",
    "        break\n",
    "    return result\n",
    "\n",
    "analyst_agent = {\"dialog_history\": lambda x: dialog_history2prompt(x[\"dialog_history\"], with_comments=True), \n",
    "              \"criterion_descr\": lambda x: criterion2prompt(x[\"criterion_descr\"]), \n",
    "              \"test_info\": lambda x: test_info2prompt(x[\"test_info\"], with_criterions=False)} | analyst_prompt | llm.with_structured_output(analyst_schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7fbd770-47b3-4dc2-aeb2-5f598308efd0",
   "metadata": {},
   "source": [
    "### Валиадция системы оценки тестирования"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ab20da67-3a43-4876-9425-971088a2c78f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "  \"test_info\": {\n",
    "    \"title\": \"Перенос срока релиза по просьбе команды: переговоры с бизнес-стейкхолдером\",\n",
    "    \"description\": \"Проверяется умение кандидата аргументированно управлять ожиданиями, коммуницировать риски и договариваться о переносе срока без потери доверия.\",\n",
    "    \"target\": \"Оценить навыки управления ожиданиями, ясность коммуникации, работу с рисками, переговоры и фиксацию договорённостей.\",\n",
    "    \"criterions\": {\n",
    "      \"Expectation Management\": \"Корректная установка и корректировка ожиданий стейкхолдера; отсутствие чрезмерных обещаний.\",\n",
    "      \"Risk Communication\": \"Чёткое и правдивое описание рисков и их влияния; указание на вероятности/неопределённости.\",\n",
    "      \"Negotiation & Options\": \"Предложение реалистичных вариантов, компромиссов и критериев принятия решения.\",\n",
    "      \"Clarity & Next Steps\": \"Структура сообщения, конкретные шаги, сроки, ответственные и каналы подтверждений.\",\n",
    "      \"Empathy & Tone\": \"Уважительный, спокойный тон, признание приоритетов бизнеса и ограничений команды.\"\n",
    "    }\n",
    "  },\n",
    "\n",
    "  \"dialog_history\": [\n",
    "    {\n",
    "      \"from\": \"assistant\",\n",
    "      \"content\": \"Нам важен релиз в следующую пятницу. Это публично озвучено клиентам. Сможете подтвердить сроки?\",\n",
    "      \"meta\": {}\n",
    "    },\n",
    "    {\n",
    "      \"from\": \"user\",\n",
    "      \"content\": \"С текущими багами по платежам есть риск не уложиться к пятнице. Предлагаю рассмотреть перенос на понедельник, чтобы не публиковать нестабильную сборку.\",\n",
    "      \"meta\": {}\n",
    "    },\n",
    "    {\n",
    "      \"from\": \"assistant\",\n",
    "      \"content\": \"Перенос звучит болезненно. Какие у нас варианты, кроме сдвига даты?\",\n",
    "      \"meta\": {}\n",
    "    },\n",
    "    {\n",
    "      \"from\": \"user\",\n",
    "      \"content\": \"Вариант 1: релизим только фиксы по безопасности и платежам, без новых фич. Вариант 2: откатываем проблемную часть и оставляем остальное. Оба требуют регресса в выходные.\",\n",
    "      \"meta\": {}\n",
    "    },\n",
    "    {\n",
    "      \"from\": \"assistant\",\n",
    "      \"content\": \"Какой риск инцидента, если релиз делать в пятницу с урезанным объёмом?\",\n",
    "      \"meta\": {}\n",
    "    },\n",
    "    {\n",
    "      \"from\": \"user\",\n",
    "      \"content\": \"Оценочно 15–20% шанс на деградацию платежей при высокой нагрузке. Можем снизить до ~5–7%, если провести нагрузочные тесты в субботу и зарезервировать on-call.\",\n",
    "      \"meta\": {}\n",
    "    },\n",
    "    {\n",
    "      \"from\": \"assistant\",\n",
    "      \"content\": \"Кто будет на on-call и как сообщим клиентам об изменениях в составе релиза?\",\n",
    "      \"meta\": {}\n",
    "    },\n",
    "    {\n",
    "      \"from\": \"user\",\n",
    "      \"content\": \"Пока не назначал on-call и коммуникацию, сначала хотел согласовать стратегию. Если окей вариант с урезанным релизом, оформлю план в Confluence и разошлю рассылку.\",\n",
    "      \"meta\": {\n",
    "        \"comments\": \"Критично: отсутствие заранее подготовленных ответственных и плана коммуникаций при уже предложенной стратегии.\"\n",
    "      }\n",
    "    },\n",
    "    {\n",
    "      \"from\": \"assistant\",\n",
    "      \"content\": \"Понял. Тогда фиксируем: в пятницу выпускаем только обязательные фиксы, в субботу нагрузочные тесты, on-call — кто именно, сроки и каналы оповещения?\",\n",
    "      \"meta\": {}\n",
    "    },\n",
    "    {\n",
    "      \"from\": \"user\",\n",
    "      \"content\": \"Назначаю on-call: Иванов и Петров, 24/7 до понедельника 10:00. Сегодня до 17:00 пришлю план с рисками, сценариями отката и шаблоном письма клиентам. Приму на себя коммуникацию с ключевыми клиентами.\",\n",
    "      \"meta\": {}\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "test_info = data[\"test_info\"]\n",
    "dialog_history = data[\"dialog_history\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "7b38ef39-4da1-43c1-bfad-b47d8014f7b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install langgraph langchain pydantic\n",
    "from __future__ import annotations\n",
    "from typing import TypedDict, Dict, Any, List\n",
    "from langgraph.graph import StateGraph, END\n",
    "\n",
    "# === ВАЖНО ===\n",
    "# Считаем, что ВЫ уже где-то выше в коде создали:\n",
    "# - analyst_agent: Chain   (ожидает keys: dialog_history, criterion_descr, test_info)\n",
    "# - judge_agent:   Chain   (ожидает keys: dialog_history, expert_results, test_info)\n",
    "#\n",
    "# А также у вас есть оригинальные хелперы (если нет — ниже есть локальный вариант для EXPERTS_BLOCK):\n",
    "# - dialog_history: raw [{\"from\": \"...\", \"content\": \"...\", \"meta\": {...}}, ...]\n",
    "# - test_info: raw {\"title\":..., \"description\":..., \"target\":..., \"criterions\": {...}}\n",
    "#\n",
    "# Требование по комментариям:\n",
    "# В dialog_history комментарии оставляет только assistant и только по КРИТИЧНЫМ ошибкам пользователя.\n",
    "\n",
    "\n",
    "# --- Если у вас отсутствует helper, который форматирует EXPERTS_BLOCK ---\n",
    "def expert_results2prompt_local(expert_results: Dict[str, Dict[str, Any]]) -> str:\n",
    "    \"\"\"\n",
    "    Формирует EXPERTS_BLOCK для judge_agent из словаря:\n",
    "    {\n",
    "      \"CriterionName\": {\"score\": int, \"reasoning\": str}, ...\n",
    "    }\n",
    "    \"\"\"\n",
    "    blocks = []\n",
    "    for name, data in expert_results.items():\n",
    "        score = data.get(\"score\")\n",
    "        reasoning = data.get(\"reasoning\", \"\")\n",
    "        blocks.append(f\"CRITERION: {name}\\nSCORE: {score}\\nJustification: {reasoning}\")\n",
    "    return \"\\n##\\n\".join(blocks)\n",
    "\n",
    "\n",
    "# --- Состояние графа ---\n",
    "class EvalState(TypedDict, total=False):\n",
    "    # Вход\n",
    "    test_info: Dict[str, Any]            # {\"title\", \"description\", \"target\", \"criterions\": {name: descr}}\n",
    "    dialog_history: List[Dict[str, Any]] # [{\"from\": \"assistant\"/\"user\", \"content\": \"...\", \"meta\": {...}}, ...]\n",
    "    # Промежуточные\n",
    "    expert_results: Dict[str, Dict[str, Any]] | None = None  # {\"criterion_name\": {\"score\": int, \"reasoning\": str}}\n",
    "    experts_block: str | None = None                  # EXPERTS_BLOCK для judge_agent\n",
    "    # Выход\n",
    "    main_judge: Dict[str, Any] | None = None          # {\"main_score\": int, \"score_reasoning\": str, \"comments\": str}\n",
    "\n",
    "\n",
    "# --- Узел: запуск аналитиков по каждому критерию ---\n",
    "def run_analysts(state: EvalState) -> EvalState:\n",
    "    test_info = state[\"test_info\"]\n",
    "    dialog_history = state[\"dialog_history\"]\n",
    "\n",
    "    criterions: Dict[str, str] = test_info.get(\"criterions\", {})\n",
    "    expert_results: Dict[str, Dict[str, Any]] = {}\n",
    "    print(\"Агенты-аналитики за работой!\")\n",
    "    for name, descr in criterions.items():\n",
    "        # ВАЖНО: analyst_agent у вас уже содержит маппинг\n",
    "        # (\"dialog_history\" -> dialog_history2prompt(with_comments=True), \n",
    "        #  \"criterion_descr\" -> criterion2prompt, \n",
    "        #  \"test_info\" -> test_info2prompt(with_criterions=False))\n",
    "        payload = {\n",
    "            \"dialog_history\": dialog_history,\n",
    "            \"criterion_descr\": {name: descr},\n",
    "            \"test_info\": test_info\n",
    "        }\n",
    "        result = analyst_agent.invoke(payload)  # -> {\"score\": int, \"reasoning\": str}\n",
    "        expert_results[name] = {\n",
    "            \"score\": result[\"score\"],\n",
    "            \"reasoning\": result[\"reasoning\"]\n",
    "        }\n",
    "        print(f\"Отработал {name} слоняра:\\n{result}\\n\\n\")\n",
    "\n",
    "    return {**state, \"expert_results\": expert_results}\n",
    "\n",
    "\n",
    "# --- Узел: подготовка EXPERTS_BLOCK для судьи ---\n",
    "def build_experts_block(state: EvalState) -> EvalState:\n",
    "    expert_results = state[\"expert_results\"]\n",
    "    experts_block = expert_results2prompt_local(expert_results)\n",
    "    return {**state, \"experts_block\": experts_block}\n",
    "\n",
    "\n",
    "# --- Узел: финальная оценка главным судьёй ---\n",
    "def run_judge(state: EvalState) -> EvalState:\n",
    "    payload = {\n",
    "        \"dialog_history\": state[\"dialog_history\"],  # judge_agent сам вызовет dialog_history2prompt (без комментариев)\n",
    "        \"expert_results\": state[\"experts_block\"],   # уже готовый EXPERTS_BLOCK (строка)\n",
    "        \"test_info\": state[\"test_info\"]\n",
    "    }\n",
    "    main_judge = judge_agent.invoke(payload)  # -> {\"main_score\": int, \"score_reasoning\": str, \"comments\": str}\n",
    "    return {**state, \"main_judge\": main_judge}\n",
    "\n",
    "\n",
    "# --- Сборка графа ---\n",
    "def build_graph():\n",
    "    graph = StateGraph(EvalState)\n",
    "    graph.add_node(\"analysts\", run_analysts)\n",
    "    graph.add_node(\"experts_prepare\", build_experts_block)\n",
    "    graph.add_node(\"judge\", run_judge)\n",
    "\n",
    "    graph.set_entry_point(\"analysts\")\n",
    "    graph.add_edge(\"analysts\", \"experts_prepare\")\n",
    "    graph.add_edge(\"experts_prepare\", \"judge\")\n",
    "    graph.add_edge(\"judge\", END)\n",
    "    return graph.compile()\n",
    "\n",
    "\n",
    "# === Пример запуска ===\n",
    "# (ниже минимальный пример; подайте сюда ваши реальные test_info и dialog_history)\n",
    "judgment_subsystem = build_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "026770cf-454e-4e9a-be8e-8580d44d6a6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Агенты-аналитики за работой!\n",
      "Отработал Expectation Management слоняра:\n",
      "{'score': 3, 'reasoning': 'Кандидат демонстрирует способность аргументированно управлять ожиданиями и коммуницировать риски. Однако, он не полностью фиксирует все условия переноса срока релиза, что может вызвать недопонимание в дальнейшем.'}\n",
      "\n",
      "\n",
      "Отработал Risk Communication слоняра:\n",
      "{'score': 3, 'reasoning': 'Кандидат четко описывает риски и их влияние, указывая на вероятности и неопределенности. Однако, он не полностью фиксирует все условия переноса релиза, такие как конкретные сроки и каналы оповещения клиентов.'}\n",
      "\n",
      "\n",
      "Отработал Negotiation & Options слоняра:\n",
      "{'score': 3, 'reasoning': 'Кандидат предложил несколько вариантов решения проблемы и оценил риски каждого из них. Однако, он не предложил реалистичных вариантов компромисса или критериев принятия решения.'}\n",
      "\n",
      "\n",
      "Отработал Clarity & Next Steps слоняра:\n",
      "{'score': 3, 'reasoning': 'Ответ пользователя структурирован и содержит конкретные шаги для выполнения задачи. Однако, ответ не полностью соответствует критерию, так как не указаны все ответственные лица и каналы подтверждений.'}\n",
      "\n",
      "\n",
      "Отработал Empathy & Tone слоняра:\n",
      "{'score': 3, 'reasoning': 'Пользователь демонстрирует уважительный и спокойный тон, признание приоритетов бизнеса и ограничений команды. Однако, он не полностью фиксирует все детали переговоров, что может вызвать недопонимание в дальнейшем.'}\n",
      "\n",
      "\n",
      "{'main_score': 3, 'score_reasoning': 'Кандидат демонстрирует умение аргументированно управлять ожиданиями, четко коммуницировать риски и предлагать реалистичные варианты решения. Однако, он не полностью фиксирует все условия переноса срока релиза, что может вызвать недопонимание в дальнейшем.', 'comments': 'Пользователь показал хорошие навыки управления ожиданиями и коммуникации рисков, но не полностью фиксировал все условия переноса срока релиза.'}\n"
     ]
    }
   ],
   "source": [
    "# Пример test_info и dialog_history должны соответствовать вашим форматам.\n",
    "\n",
    "init_state: EvalState = {\n",
    "    \"test_info\": test_info,\n",
    "    \"dialog_history\": dialog_history\n",
    "}\n",
    "\n",
    "final_state = judgment_subsystem.invoke(init_state)\n",
    "# Финальный JSON от главного судьи:\n",
    "print(final_state[\"main_judge\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d40e1e59-7cb6-4dbe-9f6f-88fc3778fd0d",
   "metadata": {},
   "source": [
    "## Система генерации промптов для агентов-экзаменаторов"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e72fb8e-f195-471e-9541-28eb9ed053bd",
   "metadata": {},
   "source": [
    "### Схемы мета-агентов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0efb08f8-2911-45d7-a629-c3b91c40dbfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_person_info(person_dataset, person_id: int | None = None, structured: bool = False) -> str | dict[str, str]:\n",
    "    person = person_dataset.iloc[person_id] if person_id else person_dataset.sample().iloc[0]\n",
    "    person = person.to_dict()\n",
    "    return person if structured else person2prompt(person)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "391c6774-dfdf-4a8a-8263-35c4b3ef00d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_person_schema = {\n",
    "\"title\": \"person-generator\",\n",
    "    \"description\": \"Ответ системы генерации личности агента-экзаменатора.\",\n",
    "    \"type\": \"object\",\n",
    "    \"properties\": {\n",
    "        \"character_info\": {\n",
    "            \"description\": \"Новое краткое описание личности агента-экзаменатора, которое имеет вид структурированного промпт-характера (описание типов личности) для имитации поведения без лишних фактов.\",\n",
    "            \"type\": \"string\",\n",
    "        },\n",
    "    },\n",
    "    \"required\": [\"character_info\"]\n",
    "}\n",
    "meta_person_prompt = ChatPromptTemplate.from_messages([(\"system\", \"\"\"Ты - система для генерации личности агента-экзаменатора, которая базируется на следующем описании человека:\n",
    "### PERSON_BLOCK\\n\n",
    "{person}\n",
    "###\n",
    "Твоя главная задача - выявить особенности характера текущего описания изменить детали личности, \n",
    "убрав несущественные факты, для соответствия данного описания человеку, который мог бы походить на роль экзаменатора.\n",
    "- Делай описания личности небольшим (1-3 коротких предложения)!\n",
    "- Не переноси напрямую все факты из PERSON_BLOCK!\n",
    "- Добавь промпт-характеру персонажа подробное описание типов характера!\n",
    "- Описания должны иметь стиль сводок - только четкая структура и минимум бесполезных фактов!\n",
    "\"\"\")])\n",
    "\n",
    "meta_person_agent = {\"person\": lambda x: person2prompt(x[\"person\"])} | meta_person_prompt | llm.with_structured_output(meta_person_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "35a501f7-6c76-48e0-ab7e-9c0dbe4ec973",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_advisor_schema = {\n",
    "\"title\": \"advisor-generator\",\n",
    "    \"description\": \"Ответ системы генерации агентов-экзаменаторов из информации о тесте (TEST_BLOCK)\",\n",
    "    \"type\": \"object\",\n",
    "    \"properties\": {\n",
    "        \"main_plan\": {\n",
    "            \"description\": \"Из описания теста выдели ключевой набор тем, которые должен раскрыть агент (по порядку)\",\n",
    "            \"type\": \"array\",\n",
    "            \"minItems\": 2,\n",
    "            \"maxItems\": 5,\n",
    "            \"items\": {\n",
    "                \"description\": \"Название темы и её суть (1 предложение)\",\n",
    "                \"type\": \"string\"\n",
    "            }\n",
    "        },\n",
    "        \"test_info\": {\n",
    "            \"description\": \"Важная нформации/контекста тестирования со стороны тестируемого пользователя (TEST_BLOCK).\\\n",
    "            Т.е. та необходимая информация, которая должна быть важна для экзаменатора по отношению к тесту, который будет проходить пользователь (не более 2-3 предложений).\",\n",
    "            \"type\": \"string\"\n",
    "        },\n",
    "    },\n",
    "    \"required\": [\"main_plan\", \"test_info\"]\n",
    "}\n",
    "\n",
    "meta_advisor_prompt = ChatPromptTemplate.from_messages([(\"system\", \"\"\"Ты - система подготовки контекста для экзаменаторов на основе информации о тесте:\n",
    "###\\n TEST_BLOCK\n",
    "{test_info}\n",
    "### \\n PERSON_BLOCK (информация о личности экзаменатора)\\n\n",
    "{person}\n",
    "###\n",
    "Тебе нужно помочь экзаменатору подготовиться к тестированию с пользователем и важно правильно составить JSON c контекстом!\"\"\")])\n",
    "\n",
    "meta_advisor_agent = {\"test_info\": lambda x: test_info2prompt(x[\"test_info\"]),\n",
    "                     \"person\": lambda x: x[\"person\"]} | meta_advisor_prompt | llm.with_structured_output(meta_advisor_schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e5e4a7-2f31-49e1-a634-079c35a07208",
   "metadata": {},
   "source": [
    "### Валидация системы генерации агентов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "190b9fa0-f5cd-428c-be63-292170ae713c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'character_info': 'Мужчина в возрасте 68 лет, бывший организатор мероприятий и волонтерский координатор. Он обладает сильными организационными навыками и практическим мышлением, что делает его отличным организатором. Он любит объединять людей и обеспечивать выполнение всех задач в срок. Его открытость к новым идеям позволяет ему эффективно использовать различные инструменты для планирования мероприятий и коммуникации. Он также увлечен историей и часто посещает музеи и исторические места с женой.'}\n"
     ]
    }
   ],
   "source": [
    "person = get_person_info(ds, person_id=1051, structured=True)\n",
    "\n",
    "person_result = meta_person_agent.invoke({\"person\": person})\n",
    "print(person_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "9025257a-9f60-476c-a451-b43b366a7f65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'main_plan': ['Перенос срока релиза по просьбе команды', 'Управление ожиданиями бизнес-стейкхолдера', 'Коммуникация рисков и переговоры', 'Фиксация договоренностей и следующих шагов'], 'test_info': 'Экзаменатор должен проверить умение кандидата аргументированно управлять ожиданиями, коммуницировать риски и договариваться о переносе срока без потери доверия.'}\n"
     ]
    }
   ],
   "source": [
    "results = meta_advisor_agent.invoke({\"test_info\": test_info, \"person\": person_result[\"character_info\"]})\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7273ba4-1ccb-4715-91e8-70b44ccf37f0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Система тестирования пользователя системы (User + Advisor + Assistant часть)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e9426e5-6cae-4935-b2e4-098072fadf12",
   "metadata": {},
   "source": [
    "### Описание схем агентов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "62676ed9-641d-4a84-b8d0-fc8e894d4798",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Any, Optional, TypedDict, Literal\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema import HumanMessage, AIMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.checkpoint.memory import MemorySaver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "abb7b8ba-2630-4969-8c10-6e2e94ad90e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plans2prompt(plans: list[str]) -> str:\n",
    "    \"\"\"\n",
    "    Ожидается список тем: []\n",
    "    \"\"\"\n",
    "    return \"\\n\".join([f\"{i}) {p}\" for i, p in enumerate(plans, start=1)] )\n",
    "    \n",
    "\n",
    "def dialog2prompt(messages: list[dict[str, str]]) -> str:\n",
    "    \"\"\"\n",
    "    Ожидается список вида: [{\"role\":\"user\"/\"assistant\", \"content\":\"...\"} , ...]\n",
    "    \"\"\"\n",
    "    lines = []\n",
    "    for m in messages:\n",
    "        role = m.get(\"role\", \"user\")\n",
    "        content = m.get(\"content\", \"\")\n",
    "        lines.append(f\"{role.upper()}: {content}\")\n",
    "    return \"\\n\".join(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "aa2482d1-7e70-4bae-bfba-8ebb86da62d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "advisor_schema = {\n",
    "    \"title\": \"advisor\",\n",
    "    \"description\": \"Ответ экзаменатора в ходе общения с пользователем с учетом проделанного плана.\",\n",
    "    \"type\": \"object\",\n",
    "    \"properties\": {\n",
    "        \"response\": {\n",
    "            \"description\": \"Реалистичный ответ пользователя от лица экзаменатора на основе истории общения с ним и планом тестирования (PLAN_BLOCK).\",\n",
    "            \"type\": \"string\",\n",
    "        },\n",
    "        \"comment\": {\n",
    "            \"description\": \"Комментарий со стороны экзаменатора на крайний/последний ответ пользователя в случае очень хорошего или очень плохого ответа. По умолчанию это поле: null\",\n",
    "            \"type\": [\"string\", \"null\"]\n",
    "        },\n",
    "        \"realised_plan\": {\n",
    "            \"description\": \"Данное поле заполняется True, ТОЛЬКО если предыдущий ответ пользователя можно считать завершением темы актуального плана.\",\n",
    "            \"type\": \"boolean\"\n",
    "        }\n",
    "    },\n",
    "    \"required\": [\"response\", \"comment\", \"realised_plan\"]\n",
    "}\n",
    "advisor_prompt = ChatPromptTemplate.from_messages([(\"system\", \"\"\"Ты - Экзаменатор (assistant) со следующим описанием личности:\n",
    "### PERSON_BLOCK\\n\n",
    "{person}\n",
    "###\n",
    "Краткая информация о тесте:\n",
    "### TEST_BLOCK\\n\n",
    "{test_info}\n",
    "###\n",
    "\n",
    "Актуальные темы по порядку (раскрываем текущую первую тему с учетом истории общения с пользователем):\n",
    "###\n",
    "{plans}\n",
    "###\n",
    "\n",
    "Твоя главная задача - провести тестирование пользователя (user) поэтапно по плану тестирования и в случае реализации всех планов тестирования быстрее завершить диалог!\n",
    "- Строго следуй своим паттернам личности и стилю общения!\n",
    "- Задавай направление беседы вокруг актуальных тем плана!\n",
    "- Отмечай поле realised_plan, если актуальная тема общения завершилась на предыдущем сообщении пользователя!\n",
    "- Отвечай на вопросы пользователя во время теста, если посчитаешь их существенными (иначе игнорируй или проси вернуться к диалогу)!\n",
    "- Если история общения пустая, то тогда начинай диалог сам и веди к первой теме сразу!\n",
    "\"\"\"), (\"assistant\", \"История общения с пользователем (user) и экзаменатором (assistant):\\n{dialog_history}\")])\n",
    "\n",
    "advisor_agent = {\"person\": lambda x: x[\"person\"], \n",
    "                 \"test_info\": lambda x: x[\"test_info\"], \n",
    "                 \"plans\": lambda x: plans2prompt(x[\"plans\"]),\n",
    "                 \"dialog_history\": lambda x: dialog2prompt(x[\"messages\"])} | advisor_prompt | llm.with_structured_output(advisor_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "8cf0ec49-d2ba-4e08-acba-f0e148b299fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "planner_schema = {\n",
    "    \"title\": \"plan-assistant\",\n",
    "    \"description\": \"Ассистент, который отмечает актуальную тему плана, формирует список 'актуальные+будущие', решает пора ли завершать диалог.\",\n",
    "    \"type\": \"object\",\n",
    "    \"properties\": {\n",
    "        \"current_plan_index\": {\n",
    "            \"description\": \"Индекс текущей темы плана (0-based). Если план завершен, укажи последний индекс.\",\n",
    "            \"type\": \"integer\"\n",
    "        },\n",
    "        \"active_topic\": {\n",
    "            \"description\": \"Номер текущей активной темы (по порядку в PLAN_BLOCK)\",\n",
    "            \"type\": \"integer\",\n",
    "            \"minimum\": 1\n",
    "        },\n",
    "        \"upcoming_topics\": {\n",
    "            \"description\": \"Список следующих тем (по порядку)\",\n",
    "            \"type\": \"array\",\n",
    "            \"items\": {\"type\": \"string\"}\n",
    "        },\n",
    "        \"should_end_dialog\": {\n",
    "            \"description\": \"True — если все темы раскрыты или пользователь явно завершил общение.\",\n",
    "            \"type\": \"boolean\"\n",
    "        },\n",
    "        \"assistant_comment\": {\n",
    "            \"description\": \"Короткий служебный комментарий в виде подсказки для пользователя, если ему требуется помощь.\",\n",
    "            \"type\": \"string\"\n",
    "        }\n",
    "    },\n",
    "    \"required\": [\"current_plan_index\", \"active_topic\", \"upcoming_topics\", \"should_end_dialog\", \"assistant_comment\"]\n",
    "}\n",
    "\n",
    "planner_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\",\n",
    "     \"Ты — Планировщик теста (assistant-planner). Твоя задача — отмечать актуальные темы плана, \"\n",
    "     \"обновлять их порядок с учетом истории и сигналить, когда диалог нужно корректно завершать.\\n\\n\"\n",
    "     \"### PERSON_BLOCK\\n{person}\\n###\\n\"\n",
    "     \"### TEST_BLOCK\\n{test_info}\\n###\\n\"\n",
    "     \"### PLAN_BLOCK (по порядку)\\n{full_plan}\\n###\\n\\n\"\n",
    "     \"GUIDE:\\n\"\n",
    "     \"- Проанализируй историю диалога.\\n\"\n",
    "     \"- Если предыдущий ответ экзаменатора имел realised_plan=True — переходи к следующей теме.\\n\"\n",
    "     \"- Если пользователь явно запросил завершение или все темы раскрыты — should_end_dialog=True.\\n\"\n",
    "     \"- Выведи structured output по schema plan-assistant.\"\n",
    "    ),\n",
    "    (\"assistant\", \"История общения (user/assistant):\\n{dialog_history}\")\n",
    "])\n",
    "\n",
    "planner_agent = {\n",
    "    \"person\": lambda x: x[\"person\"],\n",
    "    \"test_info\": lambda x: x[\"test_info\"],\n",
    "    \"full_plan\": lambda x: plans2prompt(x[\"full_plans\"]),     # полный исходный план\n",
    "    \"dialog_history\": lambda x: dialog2prompt(x[\"messages\"]),\n",
    "} | planner_prompt | llm.with_structured_output(planner_schema)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "e9245f54-285c-4fe3-81d3-2af26ab9e419",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphState(TypedDict):\n",
    "    person: str\n",
    "    test_info: str\n",
    "    full_plans: list[dict[str, str]]  # исходный полный план\n",
    "    plans: list[dict[str, str]]       # актуальные темы (пересобираются планировщиком)\n",
    "    messages: list[dict[str, str]]    # история [{role, content}], где у assistant контент = JSON (advisor_schema)\n",
    "    done: bool\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "841223b2-b3ea-4a8f-8534-40189abdf1d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def node_planner(state: GraphState) -> GraphState:\n",
    "    # вызвать планировщика\n",
    "    out = planner_agent.invoke({\n",
    "        \"person\": state[\"person\"],\n",
    "        \"test_info\": state[\"test_info\"],\n",
    "        \"full_plans\": state[\"full_plans\"],\n",
    "        \"messages\": state[\"messages\"],\n",
    "    })\n",
    "    # пересобираем актуальные планы: текущая + оставшиеся (по индексу)\n",
    "    idx = out[\"current_plan_index\"]\n",
    "    full = state[\"full_plans\"]\n",
    "    idx = min(idx, len(full)-1) if full else 0\n",
    "    new_plans = full[idx:] if idx < len(full) else []\n",
    "\n",
    "    state[\"plans\"] = new_plans\n",
    "    state[\"done\"] = bool(out[\"should_end_dialog\"])\n",
    "    # можно добавить служебное сообщение в историю (по желанию)\n",
    "    planner_note = {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": f\"[PLAN_STATE] idx={idx}; active='{out['active_topic']}'; \"\n",
    "                   f\"upcoming={out['upcoming_topics']}; done={state['done']}; \"\n",
    "                   f\"remark={out['assistant_comment']}\"\n",
    "    }\n",
    "    state[\"messages\"] = state[\"messages\"] + [planner_note]\n",
    "    return state\n",
    "\n",
    "def node_examiner(state: GraphState) -> GraphState:\n",
    "    # вызвать экзаменатора (идентичная схема advisor_schema)\n",
    "    out = examiner_agent.invoke({\n",
    "        \"person\": state[\"person\"],\n",
    "        \"test_info\": state[\"test_info\"],\n",
    "        \"plans\": state[\"plans\"],      # актуальная + будущие темы\n",
    "        \"messages\": state[\"messages\"]\n",
    "    })\n",
    "    # кладём в историю user/assistant согласно вашему формату\n",
    "    state[\"messages\"] = state[\"messages\"] + [{\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": json_dumps(out)  # сохраняем structured output как текст\n",
    "    }]\n",
    "    # если экзаменатор отметил завершение текущей темы — в следующем цикле планировщик сдвинет индекс\n",
    "    return state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "c85c4397-ae8d-460a-a1dc-8f220c021bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def should_stop(state: GraphState) -> bool:\n",
    "    # стоп если планировщик сказал done=True ИЛИ у нас пустые планы (все темы раскрыты)\n",
    "    no_more_topics = len(state.get(\"plans\", [])) == 0\n",
    "    return state.get(\"done\", False) or no_more_topics\n",
    "\n",
    "def route_after_planner(state: GraphState) -> Literal[\"END\", \"examiner\"]:\n",
    "    return \"END\" if should_stop(state) else \"examiner\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "4ce2b7f5-eb31-42d2-a5e3-ebfec36c61b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "json_dumps = lambda o: json.dumps(o, ensure_ascii=False)\n",
    "\n",
    "workflow = StateGraph(GraphState)\n",
    "workflow.add_node(\"planner\", node_planner)\n",
    "workflow.add_node(\"examiner\", node_examiner)\n",
    "\n",
    "# старт → planner\n",
    "workflow.set_entry_point(\"planner\")\n",
    "# planner → (END | examiner)\n",
    "workflow.add_conditional_edges(\n",
    "    \"planner\",\n",
    "    route_after_planner,\n",
    "    {\n",
    "        \"END\": END,\n",
    "        \"examiner\": \"examiner\"\n",
    "    }\n",
    ")\n",
    "# examiner → обратно к planner\n",
    "workflow.add_edge(\"examiner\", \"planner\")\n",
    "\n",
    "memory = MemorySaver()  # чекпоинты (по желанию)\n",
    "app = workflow.compile(checkpointer=memory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3bec12b-c61d-438a-887a-5baf53ed927b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
